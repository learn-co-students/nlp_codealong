{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# NLP Codealong"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.student_caller import one_random_student\n", "from src.student_list import quanggang"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_20newsgroups\n", "\n", "# Import our best friends\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = fetch_20newsgroups()\n", "data.keys()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.target_names"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id='eda'></a>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## EDA"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As always, we want to look at the basic shape of the data.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = pd.DataFrame(data['data'])\n", "X.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What form do we want the above dataframe to take? What does a row represent? What does a column represent?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# your answer here"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's take a look at one record.  What type of preprocessing steps should we take to isolate tokens of high semantic value?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X.iloc[3].values[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Answer here"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Frequency Distributions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at the frequency distribution of all the words in the corpus.  To do so, we will use the FreqDist class from nltk.  \n", "\n", "The FreqDist methods expect to receive a list of tokens, so we need to do a little preprocessing. We will use the RegexpTokenizer from nltk.  \n", "\n", "There are a few places in this notebook where regular expressions will prove useful. \n", "\n", "Let's look at this tool [regexr](https://regexr.com/) and try to figure out the very basic pattern to match any word.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate a RegexpTokenizer object and pass that pattern as the pattern argument\n", "\n", "from nltk.probability import FreqDist\n", "from nltk.tokenize import RegexpTokenizer \n", "\n", "rt = RegexpTokenizer()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Join all of the words \n", "all_docs = ' '.join(list(X_train[0]))\n", "\n", "# use the rt object's tokenize method to create a list of all of the tokens\n", "all_words = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate a FreqDist object and pass allwords into it\n", "\n", "# use the most_common method to see the 10 most common words\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualize the distribution of the target with a bar chart"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = data['target']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Target classes\n", "data['target_names']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Bar Chart Here (horizontal, preferably)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Quick Model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our model validation principles are consistent with NLP modeling.   \n", "We split our data in the same way, ideally with a hold out set.   \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train Test Split\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["X_train[0]"]}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": ["## Count Vectorizor\n", "\n", "A count vectorizor takes as input all of the documents in their raw form.  That being the case, if we are doing any preprocessing, such as custom transformations like lemming and stemming, we will need to recombine the tokens into the original documents.  \n", "\n", "For our FSM, we will pass our documents into the vectorizer in their raw form."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "\n", "# instantiate a CountVectorizor object \n", "cv = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Question: \n", "\n", "Look at all those wonderful parameters.  What parameters would be useful to test out? "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at our regular expressions again, and add a better pattern.\n", "\n", "[regexr](https://regexr.com/)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate a better CountVectorizer with stopwords, a regular expression pattern, and whatever else you would like  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our CountVectorizer, we apply the same principles of model validation as we have with other data.  Fit on the training set, and transform both the train and test with that fit object. This will create a vocabulary associated with high predictive value built off of the training vocabulary. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cv.fit_transform(X_train[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### DataFrame from sparse and get feature names"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As we see above, the fit_transform method returns a sparse matrix.  Luckily, our alogrithms will handle sparse matrices, as we will see below.  But, if we want, we can convert our sparse matrix to a fully expressed dataframe using the .from_spmatrix method taken from DataFrame.sparse"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# convert the sparse matrix from above to a dataframe\n", "X_train_vec = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also add the words as column names using cv.get_feature_names()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add words as column names"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As mentioned above, we don't necessarily need the feature names present to build our model.\n", "\n", "Let's build a model with the count vectorizer from above, and use sklearns pipeline and cross_validate to see how accurately we can classify the documents.\n", "\n", "We will apply a CountVectorizor and then a multinomial naive bayes classifier."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import make_pipeline\n", "# import MultinomialNB\n", "# import cross_validate\n", "\n", "# create a pipeline object with our CountVectorizer and Multinomial Naive Bayes as our steps\n", "\n", "# feed the pipeline into cross_validate along with X_train[0] and y_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(quanggang)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have a funcitonal pipeline, we have the framework to easily test out new parameters and models. Try n-grams, min_df/max_df, tfidf vectorizers, better token patterns.  Try Random Forests, XGBoost, and SVM's. The world is your oyster."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![MrBean_oysters](https://media.giphy.com/media/KZepR2JrdDbI0NYVMs/giphy.gif)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Of course, when we are finished tuning our model, we fit on the entire training set, and score on the test.\n", "fsm_pipe.fit(X_train[0], y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_hat_test = fsm_pipe.predict(X_test[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import plot_confusion_matrix\n", "\n", "plot_confusion_matrix(fsm_pipe,X_test[0], y_test)"]}], "metadata": {"kernelspec": {"display_name": "learn-env", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}